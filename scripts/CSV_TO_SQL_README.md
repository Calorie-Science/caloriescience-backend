# CSV to SQL Converter for Simple Ingredients

This script converts a CSV file containing nutritional data into SQL INSERT statements for the `simple_ingredients` table.

## Features

- **Batch Processing**: Generates efficient batch INSERT statements (default 100 rows per batch)
- **Data Validation**: Validates and properly formats all data types (decimals, booleans, arrays)
- **NULL Handling**: Properly handles NULL/empty values
- **Array Fields**: Converts comma-separated strings to PostgreSQL array format
- **SQL Injection Safe**: Escapes all string values properly
- **Conflict Handling**: Uses `ON CONFLICT (name) DO NOTHING` to skip duplicates
- **Progress Tracking**: Shows real-time progress during conversion

## Requirements

- Python 3.6+
- No external dependencies (uses only Python standard library)

## CSV Format

Your CSV file should have the following columns:

```
fdc_id, name, display_name, category, serving_quantity, serving_unit,
calories, protein_g, carbs_g, fat_g, fiber_g, sugar_g,
saturated_fat_g, trans_fat_g, cholesterol_mg,
vitamin_a_mcg, vitamin_d_mcg, vitamin_e_mg, vitamin_k_mcg,
vitamin_c_mg, thiamin_mg, riboflavin_mg, niacin_mg,
vitamin_b6_mg, vitamin_b12_mcg, folate_mcg, biotin_mcg,
pantothenic_acid_mg, choline_mg,
calcium_mg, phosphorus_mg, magnesium_mg, sodium_mg,
potassium_mg, chloride_mg, iron_mg, zinc_mg, copper_mg,
selenium_mcg, iodine_mcg, manganese_mg, molybdenum_mcg, chromium_mcg,
health_labels, diet_labels, allergens, image_url, is_active,
created_at, updated_at
```

**Notes:**
- `fdc_id` will be ignored (table uses UUID primary key)
- `created_at` and `updated_at` are auto-generated by the database
- Array fields (`health_labels`, `diet_labels`, `allergens`) should be comma-separated strings

## Usage

### Basic Usage

```bash
python3 scripts/csv_to_ingredients_sql.py your_file.csv
```

This will create `ingredients_insert.sql` in the current directory.

### Specify Output File

```bash
python3 scripts/csv_to_ingredients_sql.py your_file.csv output.sql
```

### Specify Batch Size

```bash
python3 scripts/csv_to_ingredients_sql.py your_file.csv output.sql 200
```

Larger batch sizes = fewer INSERT statements but larger file size.

## Example

```bash
# Convert ingredients.csv with default settings
python3 scripts/csv_to_ingredients_sql.py ingredients.csv

# Convert with custom output and batch size of 50
python3 scripts/csv_to_ingredients_sql.py ingredients.csv database/seeds/ingredients.sql 50
```

## Output

The script generates SQL file with:

1. **Header**: Metadata about the conversion
2. **Transaction**: Wrapped in `BEGIN` and `COMMIT`
3. **Batch INSERTs**: Multiple rows per INSERT statement
4. **Conflict Resolution**: `ON CONFLICT (name) DO NOTHING`

Example output:

```sql
-- Generated SQL for simple_ingredients table
-- Source: ingredients.csv
-- Date: 2025-11-04 10:30:00

BEGIN;

INSERT INTO simple_ingredients (name, display_name, category, ...)
VALUES
  ('apple', 'Apple (1 medium)', 'fruit', ...),
  ('banana', 'Banana (1 medium)', 'fruit', ...),
  ...
ON CONFLICT (name) DO NOTHING;

COMMIT;
```

## Running the SQL

After generating the SQL file, you can run it against your database:

### Using psql

```bash
psql -U your_user -d your_database -f output.sql
```

### Using Supabase

```bash
# If you're using Supabase, you can use their CLI
supabase db push --file output.sql
```

### Copy-Paste

For smaller files, you can copy-paste the SQL directly into Supabase SQL Editor or any PostgreSQL client.

## Data Type Mapping

| CSV Column Type | PostgreSQL Type | Notes |
|----------------|-----------------|-------|
| `int` | `DECIMAL(10, 2)` or `INTEGER` | Numbers are validated |
| `float` | `DECIMAL(10, 2)` | Converted to decimal strings |
| `varchar` | `VARCHAR(255)` | Single quotes escaped |
| `varchar(1)` (arrays) | `TEXT[]` | Comma-separated â†’ PostgreSQL array |
| `bit` | `BOOLEAN` | Converts 0/1/true/false |
| `date/datetime` | Auto-generated | Uses `NOW()` |

## Error Handling

The script will:
- Skip rows missing required fields (`name` or `category`)
- Log warnings for skipped rows
- Continue processing remaining rows
- Show summary statistics at the end

## Performance

For a 1.2 MB CSV file (~5,000-10,000 rows):
- Conversion time: ~1-5 seconds
- Output SQL file: ~2-4 MB (depending on batch size)
- Database import time: ~2-10 seconds (depending on server)

## Troubleshooting

### "No module named 'csv'"
- You're using Python 3+, csv is built-in. Check your Python installation.

### "File not found"
- Ensure the CSV file path is correct
- Use absolute path if relative path doesn't work

### SQL Import Fails
- Check for syntax errors in generated SQL
- Ensure table exists: `CREATE TABLE simple_ingredients ...`
- Verify database connection

### Duplicate Key Errors
- Script uses `ON CONFLICT (name) DO NOTHING`
- Duplicates are automatically skipped
- Check if you want `DO UPDATE` instead

## Customization

To modify the script behavior, edit `csv_to_ingredients_sql.py`:

- **Change batch size default**: Modify line with `batch_size: int = 100`
- **Add more validations**: Edit `generate_insert_statement()` function
- **Change conflict resolution**: Modify `ON CONFLICT` clause in `write_batch()`

## License

This script is part of the CalorieScience application.
